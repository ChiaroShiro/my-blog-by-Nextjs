---
title: 'MoE与强化学习笔记'
date: '2025-05-15'
tags: [ 'RL', 'AI', 'LLM', '笔记' ]
---

## OLMOE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS

> ICLR 2025

用 MoE 技术构建了一个小模型，总参数7B，激活1B，但是跑的效果十分之好。具体对微调、MoE的专家选择做了优化，分析了很多路由数据来更好的路由并复杂均衡。代码开源

微调时用了强化学习的 DPO 方法，因为模型很小，所以16位参数可以用14G内存跑在个人电脑上

## MOE++: ACCELERATING MIXTURE-OF-EXPERTS METHODS WITH ZERO-COMPUTATION EXPERTS

> ICLR 2025

在原始MoE基础上加了些新东西，主要是加了三个零计算量层

- 零专家（Zero Expert）：直接丢弃某些 token 的计算；

- 复制专家（Copy Expert）：跳过当前层，将上一层的输出直接复制作为本层输出；

- 常量专家（Constant Expert）：将 token 以可学习的常量向量替换，完成“替代”操作。

并且在路由层引入了残差机制，就是说上一层的门控选择会影响这次的选择

可以模型质量超过原模型的基础上加快速度。不过和强化学习似乎没啥关系

## LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing

> NeurIPS 2024

一个注重训练优化的论文，用局部敏感哈希算法对tokens进行聚类，并通过只传输聚类中心点来了减少通信量，并加入了一些比如残差补偿的优化 tricks。和推理优化似乎没啥关系

但是 P3 中提到强化学习是一种比较常见的门控方法，但给出的三个参考引用都是 2020 年前的老古董

## MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks

> NeurIPS 2024

这个论文主要关注的是通过训练密集模型再微调到 MoE 的方法，不过在里面他还提出了一个 MoE 推理的优化

P6 的 3.2 里提到了一个双路 MoE 路由的方法，简要来说就是根据 token 的重要性不同分成两种，一种 tokens 走的是核心路径，会有比较少但是参数规模比较大的层去处理，另一种走的是通用路径，会有比较多但是参数规模相应变小的层去处理这个 tokens

## MoEUT: Mixture-of-Experts Universal Transformers

> NeurIPS 2024

提出了一种将通用Transformer（UT）结合MoE的方法，相较于以前有了很多优势，而且节省了内存使用。

论文里说通过MoE和UT混合的方式实现了降低内存的效果，但是似乎没有找到内存的管理方式，也不知道是为什么能降低内存的，很奇怪，不太懂

## ADA-K ROUTING: BOOSTING THE EFFICIENCY OF MOE-BASED LLMS

> ICLR 2025

提出了 Ada-K 路由方法，与传统 MoE 固定选 k 个不同，这个论文里的新方法要根据 token 选择 k 的大小，复杂的 token 要激活更多的专家，信息少的 token 要少激活点专家。这玩意儿好像很适合用强化学习搞，论文里也是用的 PPO 的强化学习方法进行的。

具体来说就是在传统 MoE 的 Gate 前加一层强化学习层，输出 k 大小的概率分布

## SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement Learning in Continuous Control Tasks

> AAAI 2025

机器人操作等连续控制领域强化学习应用比较广泛，但是强化学习的结果是不可解释的，这个论文就是研究怎么让他可解释，用的方法是 MoE。

具体来说就是用强化学习训练整个 MoE 架构，包括专家和门控，其中门控是可解释的，这样在跑的时候就可以根据每次门控选择的结果建立决策树之类的东西来解释整个过程。

## AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for Efficient MoE Inference

很仔细地从头到尾读了一下，基于之前 2023 年那篇当时 SOTA 的 pre-gate MoE 方法（很简单的训练了一个预测后续几层专家选取的预门控方法，然后计算和预取并行），他提出了三个很显然的 MoE 预取影响因素：

- 激活的专家越少，需要加载的数据就越少。

- 如果能准确预测下一层需要哪些专家并提前加载，就可以将数据传输的延迟隐藏在当前层的计算时间中。

- 如果需要的专家已经存在于GPU缓存中，就无需加载。

然后是三个不那么显然的观察：

- 不同层、不同 token 对专家的需求是动态变化的，有些情况下，一个专家就足够了，不需要激活多个。

- 由于模型中的残差连接，相邻层之间的输入激活值具有高度相似性。这为准确预测后续层所需的专家提供了可能。

- 不同层的预取准确率和专家激活数量不同，导致它们对GPU缓存大小的需求也是不均衡的。

因此基于这三个观察提出了三个解决方案，也就是这篇论文的核心：

- 动态地为每个令牌在每一层决定激活一个还是多个专家 。

引入了“敏感度”分析。它通过计算Hessian矩阵来衡量减少专家数量对模型最终输出的影响。如果分析表明，只使用得分最高的那个专家对结果的扰动在一个预设的阈值T以下，那么系统就只激活这一个专家。这种方法考虑了不同层的重要性，允许在不牺牲模型精度的前提下，更大幅度地减少专家激活数量。

整个过程非常的数学，看不懂，不过大体意思懂了

- 准确预测并提前加载后续层所需的专家 。

利用相邻层激活值高度相似的特性，**直接使用后一层的门控网络来预测其所需的专家**，如果下一层的专家已在缓存中，系统会提前加载更后面层级的专家。

另外对一层特殊处理，AdapMoE引入了一个辅助的预测层，它利用上一个令牌最后一层的激活值来预测当前令牌第一层所需的专家。这一层是一个单独训练的传统门控层。

怎么说呢...感觉很暴力啊，这竟然就能 work 么

-  优化有限的GPU缓存资源分配，给每层预取不同数量的专家，以最大化缓存命中率 。

由于自适应门控和预取的存在，不同层对缓存的需求变得不均衡。例如，预取准确率低的层需要提前放入更多的专家，用更多缓存来弥补，这样才能提高命中率，进而大大加速。

论文将这个问题建模为一个背包问题，背包体积是GPU缓存大小，然后经典背包来求每一层的最优缓存大小。具体来说就是每一层可以选择放入几个专家，对应有大小和价值（缓存命中率），对于 L 层每层 N 个专家来说其实就是一个 L 组，每组 N 选 1 的级经典分组背包问题。每次离线计算出来以后直接把配置给这个模型。

感觉很神秘，不过这个分组背包转换还是很巧妙的。

其他一些比较有意思的点：

论文还对比了有无第一层单独门控的实验，然后对比出来是有必要的，可能这一层的门控确实比较唐突吧，通过实验来强调合理性。

还进行了消融实验，就是把三个优化点进行了 $2^3$ 的排列组合，然后得出每个部分都是有效的结论。

感觉是第一层这么认真的读完一篇论文，背景知识比较齐全之后读起来舒服多了也通常多了，还挺有意思的，有些思路很暴力很神秘，有些也挺有意思的